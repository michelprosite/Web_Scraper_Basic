{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao Web Scraping com Python\n",
    "\n",
    "## Michel Souza Santana\n",
    "\n",
    "* Data de início: 09 de maio de 2023\n",
    "\n",
    "* Duração: 60 horas\n",
    "\n",
    "> Descrição: \n",
    "\n",
    "Este curso irá ensinar a você os fundamentos do Web Scraping utilizando a linguagem de programação Python. Com o aumento da quantidade de dados disponíveis na internet, a habilidade de coletar e analisar informações da web é cada vez mais valorizada no mercado de trabalho. Neste curso, você irá aprender desde os conceitos básicos de HTML e CSS até a utilização de bibliotecas como requests, Beautiful Soup e Selenium para coletar e analisar dados de páginas web. Além disso, também iremos abordar boas práticas de Web Scraping ético e o armazenamento de dados. Ao final do curso, você estará pronto para desenvolver seu próprio Web Scraper e aplicar seus conhecimentos em projetos reais.\n",
    "\n",
    "> Autor: \n",
    "\n",
    "Michel Souza Santana é formado em Análise e Desenvolvimento de Sistemas e atua na área de Engenharia de Dados há mais de 5 anos, com experiência em coleta e análise de dados de diferentes fontes. Além disso, é entusiasta da tecnologia e das possibilidades que a análise de dados pode trazer para as empresas e para a sociedade em geral.\n",
    "\n",
    "Este curso é ideal para estudantes, analistas de dados, desenvolvedores e qualquer pessoa que queira aprender a coletar e analisar informações da web de forma automatizada. Não é necessário conhecimento prévio em Web Scraping ou Python, mas é recomendado que você tenha um conhecimento básico em programação."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Índice:\n",
    "\n",
    "* 1 - Introdução ao Web Scraping\n",
    "    * 1.1 O que é Web Scraping?\n",
    "    * 1.2 Por que é importante?\n",
    "    * 1.3 Ferramentas para Web Scraping\n",
    ">...   \n",
    "\n",
    "* 2 - HTML básico e CSS\n",
    "    * 2.1 Estrutura de uma página HTML\n",
    "    * 2.2 Tags HTML básicas\n",
    "    * 2.3 CSS para estilização de páginas\n",
    ">...\n",
    "\n",
    "* 3 - Requisições HTTP e Biblioteca requests\n",
    "    * 3.1 O que é uma requisição HTTP?\n",
    "    * 3.2 Biblioteca requests em Python\n",
    "    * 3.3 Fazendo uma requisição HTTP em Python\n",
    "    * 3.4 Tratando erros com try/except\n",
    ">...\n",
    "\n",
    "* 4 - Analisando HTML com Beautiful Soup\n",
    "    * 4.1 O que é Beautiful Soup?\n",
    "    * 4.2 Analisando HTML com Beautiful Soup\n",
    "    * 4.3 Buscando elementos HTML específicos\n",
    "    * 4.4 Tratando erros com try/except\n",
    ">...\n",
    "\n",
    "* 5 - Selenium para Web Scraping Dinâmico\n",
    "    * 5.1 O que é Web Scraping dinâmico?\n",
    "    * 5.2 Como o Selenium pode ajudar?\n",
    "    * 5.3 Automatizando tarefas com o Selenium\n",
    "    * 5.4 Tratando erros com try/except\n",
    ">...\n",
    "\n",
    "* 6 - Armazenamento de dados\n",
    "    * 6.1 Formatos de dados comuns\n",
    "    * 6.2 Armazenando dados em um arquivo CSV\n",
    "    * 6.3 Armazenando dados em um banco de dados\n",
    "    * 6.4 Tratando erros com try/except\n",
    ">...\n",
    "\n",
    "* 7 - Introdução ao Web Scraping Ético\n",
    "    * 7.1 O que é Web Scraping ético?\n",
    "    * 7.2 Legislação sobre Web Scraping\n",
    "    * 7.3 Boas práticas de Web Scraping ético\n",
    ">...\n",
    "\n",
    "* 8 - Projeto Final\n",
    "    * 8.1 Desenvolvendo um Web Scraper completo\n",
    "    * 8.2 Implementando o armazenamento de dados\n",
    "    * 8.3 Tratando erros com try/except\n",
    "    * 8.4 Boas práticas de Web Scraping ético\n",
    ">..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Introdução ao Web Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 O que é Web Scraping?\n",
    "\n",
    "Web Scraping é uma técnica utilizada para extrair informações de páginas da web de forma automatizada. Ela permite que os dados sejam coletados de maneira mais rápida e eficiente do que se fosse feito manualmente.\n",
    "\n",
    "Imagine que você precisa coletar informações de diversos sites para realizar uma pesquisa de mercado. Fazer isso manualmente seria uma tarefa demorada e tediosa, pois você teria que entrar em cada página, encontrar as informações desejadas e copiá-las para um documento.\n",
    "\n",
    "Com o Web Scraping, é possível automatizar todo esse processo. Você pode escrever um programa que acesse as páginas da web, procure as informações desejadas e as salve em um formato estruturado, como um arquivo CSV ou um banco de dados.\n",
    "\n",
    "Existem diversas ferramentas e bibliotecas que podem ser utilizadas para aplicar a técnica de Web Scraping, como a biblioteca requests e a biblioteca Beautiful Soup em Python, além do Selenium para Web Scraping dinâmico.\n",
    "\n",
    "Há inúmeros exemplos em que o Web Scraping pode ser aplicado. Uma das aplicações mais comuns é a coleta de informações de sites de e-commerce para análise de preços e concorrência. Imagine que você é proprietário de uma loja virtual e precisa manter seus preços competitivos em relação aos concorrentes. Com o Web Scraping, é possível coletar automaticamente os preços dos produtos de seus concorrentes e analisá-los em um formato estruturado, permitindo que você tome decisões estratégicas com base nessas informações.\n",
    "\n",
    "Outro exemplo de aplicação do Web Scraping é na área de análise de dados e pesquisa de mercado. Muitas empresas precisam coletar informações de diferentes fontes para realizar análises de mercado e identificar tendências. Com o Web Scraping, é possível automatizar esse processo e coletar as informações desejadas de maneira mais rápida e eficiente.\n",
    "\n",
    "É importante ressaltar que o Web Scraping deve ser feito com responsabilidade e ética, respeitando as políticas de privacidade dos sites e a legislação vigente. Por isso, é fundamental entender as boas práticas de Web Scraping ético e aplicá-las em seus projetos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Por que é importante?\n",
    "\n",
    "Web Scraping é uma técnica poderosa que pode ajudar a coletar e analisar informações de uma ampla variedade de fontes na web. Ele permite que você automatize tarefas que seriam demoradas ou impossíveis de realizar manualmente, tornando o processo de coleta de dados mais rápido, eficiente e preciso.\n",
    "\n",
    "A capacidade de extrair informações da web pode ser extremamente valiosa em diversos setores, incluindo negócios, pesquisa científica, análise de mercado, jornalismo, entre outros. Por exemplo, em uma empresa de análise de dados, o Web Scraping pode ser usado para coletar informações de diferentes fontes, como sites de notícias, blogs e redes sociais, e integrá-las em um formato estruturado para análise. Isso pode ajudar a empresa a identificar tendências, padrões e insights importantes que podem ser usados para tomar decisões mais informadas e baseadas em dados.\n",
    "\n",
    "Além disso, o Web Scraping pode ser usado para automatizar processos em diferentes setores. Por exemplo, em uma loja online, o Web Scraping pode ser usado para coletar informações sobre preços e estoques de produtos de concorrentes e atualizar automaticamente os preços e estoques da loja em conformidade. Isso pode ajudar a loja a manter seus preços competitivos e aumentar suas vendas.\n",
    "\n",
    "No entanto, é importante lembrar que o Web Scraping também apresenta desafios. Por exemplo, é preciso lidar com grandes volumes de dados, bem como questões legais e éticas, como violação de direitos autorais e privacidade. É importante estar ciente desses desafios e estar em conformidade com as leis e regulamentos aplicáveis ao usar o Web Scraping."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Ferramentas para Web Scraping\n",
    "\n",
    "As ferramentas para Web Scraping são importantes porque facilitam o processo de extração de dados de uma página da web. Existem diversas opções disponíveis no mercado, desde ferramentas simples que podem ser usadas por iniciantes até ferramentas mais avançadas que oferecem uma ampla gama de recursos.\n",
    "\n",
    "Algumas das ferramentas mais populares para Web Scraping incluem BeautifulSoup, Scrapy, Selenium e Requests. Cada uma dessas ferramentas tem suas próprias vantagens e desvantagens, e é importante escolher a que melhor se adapta às suas necessidades específicas.\n",
    "\n",
    "O BeautifulSoup, por exemplo, é uma biblioteca Python que é amplamente utilizada para analisar e extrair dados de páginas HTML e XML. É conhecida por sua facilidade de uso e flexibilidade, tornando-a uma escolha popular entre os desenvolvedores.\n",
    "\n",
    "O Scrapy, por outro lado, é um framework mais avançado que oferece muitos recursos para automatizar a extração de dados da web. É particularmente útil para a extração de grandes quantidades de dados de sites complexos.\n",
    "\n",
    "O Selenium é uma ferramenta útil para Web Scraping dinâmico, ou seja, para extrair dados de páginas que mudam dinamicamente. Ele simula a interação humana com o navegador, permitindo que você extraia dados de páginas que não podem ser facilmente acessadas usando ferramentas convencionais.\n",
    "\n",
    "Já o Requests é uma biblioteca Python simples que permite enviar solicitações HTTP e obter informações de uma página da web. É uma escolha popular para tarefas simples de Web Scraping e é fácil de usar.\n",
    "\n",
    "Independentemente da ferramenta escolhida, é importante lembrar que o Web Scraping deve ser realizado de forma ética e respeitando as leis e direitos autorais."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - HTML básico e CSS\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Estrutura de uma página HTML\n",
    "\n",
    "A estrutura de uma página HTML (Hypertext Markup Language) é fundamental para entender como realizar o Web Scraping em uma página. O HTML é a linguagem de marcação usada para criar páginas web e é composto por elementos que fornecem informações sobre a estrutura e o conteúdo da página.\n",
    "\n",
    "A estrutura básica de uma página HTML inclui a tag < html>, que define o início e o fim do documento HTML, e a tag < body>, que contém o conteúdo visível da página, como texto, imagens, links e outros elementos.\n",
    "\n",
    "Dentro do < body> é possível incluir outras tags HTML que irão definir a estrutura do conteúdo da página. Por exemplo, a tag < header> é usada para definir o cabeçalho da página, enquanto a tag < footer> é usada para definir o rodapé.\n",
    "\n",
    "Além disso, é possível usar as tags < div> e < span> para agrupar elementos em blocos ou em linhas. Esses elementos podem ser estilizados com CSS (Cascading Style Sheets) para melhorar a aparência da página.\n",
    "\n",
    "Por exemplo, imagine que você deseje extrair informações de um site de notícias. Ao analisar a estrutura da página HTML, você pode identificar as tags usadas para exibir os títulos das notícias, a data de publicação e o corpo da matéria. Com essas informações, é possível criar um script de Web Scraping que coleta as notícias e as armazena em um arquivo CSV ou em um banco de dados.\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exemplo de código de uma págia html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n<!DOCTYPE html>\\n<html>\\n<head>\\n\\t<title>Título da Página</title>\\n\\t<meta charset=\"UTF-8\">\\n\\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n\\t<link rel=\"stylesheet\" type=\"text/css\" href=\"estilos.css\">\\n</head>\\n<body>\\n\\t<header>\\n\\t\\t<h1>Meu Site</h1>\\n\\t\\t<nav>\\n\\t\\t\\t<ul>\\n\\t\\t\\t\\t<li><a href=\"#\">Home</a></li>\\n\\t\\t\\t\\t<li><a href=\"#\">Sobre</a></li>\\n\\t\\t\\t\\t<li><a href=\"#\">Contato</a></li>\\n\\t\\t\\t</ul>\\n\\t\\t</nav>\\n\\t</header>\\n\\t<main>\\n\\t\\t<section>\\n\\t\\t\\t<h2>Seção 1</h2>\\n\\t\\t\\t<p>Conteúdo da seção 1.</p>\\n\\t\\t</section>\\n\\t\\t<section>\\n\\t\\t\\t<h2>Seção 2</h2>\\n\\t\\t\\t<p>Conteúdo da seção 2.</p>\\n\\t\\t</section>\\n\\t</main>\\n\\t<footer>\\n\\t\\t<p>Direitos reservados &copy; 2023</p>\\n\\t</footer>\\n</body>\\n</html>\\n\\n'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "\t<title>Título da Página</title>\n",
    "\t<meta charset=\"UTF-8\">\n",
    "\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "\t<link rel=\"stylesheet\" type=\"text/css\" href=\"estilos.css\">\n",
    "</head>\n",
    "<body>\n",
    "\t<header>\n",
    "\t\t<h1>Meu Site</h1>\n",
    "\t\t<nav>\n",
    "\t\t\t<ul>\n",
    "\t\t\t\t<li><a href=\"#\">Home</a></li>\n",
    "\t\t\t\t<li><a href=\"#\">Sobre</a></li>\n",
    "\t\t\t\t<li><a href=\"#\">Contato</a></li>\n",
    "\t\t\t</ul>\n",
    "\t\t</nav>\n",
    "\t</header>\n",
    "\t<main>\n",
    "\t\t<section>\n",
    "\t\t\t<h2>Seção 1</h2>\n",
    "\t\t\t<p>Conteúdo da seção 1.</p>\n",
    "\t\t</section>\n",
    "\t\t<section>\n",
    "\t\t\t<h2>Seção 2</h2>\n",
    "\t\t\t<p>Conteúdo da seção 2.</p>\n",
    "\t\t</section>\n",
    "\t</main>\n",
    "\t<footer>\n",
    "\t\t<p>Direitos reservados &copy; 2023</p>\n",
    "\t</footer>\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tags HTML \n",
    "\n",
    "As tags HTML são elementos fundamentais para a construção de páginas web. Elas definem a estrutura e o conteúdo da página, permitindo que os navegadores interpretem e exibam o conteúdo de forma correta. Algumas das tags HTML básicas incluem:\n",
    "\n",
    "< html>: Define o início e o fim do documento HTML.\n",
    "\n",
    "< head>: Contém informações sobre o documento, como o título e o CSS.\n",
    "\n",
    "< body>: Define o conteúdo principal da página.\n",
    "\n",
    "< h1> a < h6>: São utilizadas para criar títulos e subtítulos na página.\n",
    "\n",
    "< p>: É utilizada para criar parágrafos na página.\n",
    "\n",
    "< a>: É utilizada para criar links na página.\n",
    "\n",
    "< img>: É utilizada para inserir imagens na página.\n",
    "\n",
    "< ul> e < ol>: São utilizadas para criar listas não-ordenadas e ordenadas, respectivamente.\n",
    "\n",
    "< li>: É utilizada para criar itens de lista.\n",
    "\n",
    "Essas são apenas algumas das tags HTML básicas, existem muitas outras que podem ser utilizadas para criar páginas web mais complexas e ricas em conteúdo.\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 CSS para estilização de \n",
    "\n",
    "O CSS (Cascading Style Sheets) é uma linguagem utilizada para estilizar páginas web. Ela permite definir como os elementos HTML devem ser exibidos na página, controlando aspectos como cores, fontes, tamanhos, posicionamento e outros estilos visuais.\n",
    "\n",
    "O CSS funciona em conjunto com o HTML, onde cada elemento HTML pode ser associado a uma ou mais regras CSS. Essas regras são definidas em um arquivo separado do HTML, chamado de arquivo CSS, ou diretamente no HTML através do atributo \"style\".\n",
    "\n",
    "As regras CSS são compostas por um seletor e um conjunto de declarações. O seletor define qual elemento HTML será estilizado e as declarações definem as propriedades que serão aplicadas ao elemento. Por exemplo, para estilizar todos os parágrafos de um documento HTML, pode-se utilizar o seguinte código CSS:\n",
    "\n",
    "css\n",
    "\n",
    "Copy code\n",
    "\n",
    "p {\n",
    "\n",
    "    font-size: 16px;\n",
    "\n",
    "    color: #333;\n",
    "\n",
    "    line-height: 1.5;\n",
    "    \n",
    "}\n",
    "\n",
    "Nesse exemplo, o seletor é a tag < p>, e as declarações definem que o tamanho da fonte será de 16 pixels, a cor do texto será um tom de cinza (#333) e o espaçamento entre linhas será de 1.5 vezes o tamanho da fonte.\n",
    "\n",
    "Existem diversas propriedades CSS disponíveis, permitindo estilizar os elementos HTML de diversas formas e criar páginas web visualmente atraentes e personalizadas. Alguns exemplos de propriedades CSS incluem:\n",
    "\n",
    "background-color: define a cor de fundo do elemento;\n",
    "font-family: define a fonte utilizada pelo elemento;\n",
    "text-align: define o alinhamento do texto dentro do elemento;\n",
    "margin e padding: definem espaçamentos internos e externos do elemento.\n",
    "Com o uso do CSS, é possível criar layouts complexos, aplicar animações e efeitos visuais, e adaptar a página para diferentes tamanhos de tela e dispositivos. É uma ferramenta essencial para qualquer desenvolvedor web."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Requisições HTTP e Biblioteca requests\n",
    "\n",
    "A comunicação entre um navegador e um servidor acontece através do protocolo HTTP (Hypertext Transfer Protocol), que é a base da World Wide Web. Para acessar uma página na web, o navegador envia uma solicitação (requisição) HTTP para o servidor, que responde com uma mensagem de resposta.\n",
    "\n",
    "O Python oferece uma biblioteca chamada \"Requests\", que simplifica o processo de envio de requisições HTTP e o tratamento de respostas. Com a biblioteca Requests, podemos automatizar esse processo de envio de requisições e tratamento de respostas, facilitando a obtenção de informações da web.\n",
    "\n",
    "A biblioteca Requests é um pacote que podemos instalar facilmente usando pip (gerenciador de pacotes do Python) e importar para nossos scripts Python. Uma vez importada, podemos usar suas funções e métodos para enviar requisições HTTP, lidar com cookies, autenticação, entre outros.\n",
    "\n",
    "Vamos ver um exemplo simples de como fazer uma requisição HTTP usando a biblioteca Requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"pt-BR\"><head><meta content=\"text/html; charset=UTF-8\" http-equiv=\"Content-Type\"><meta content=\"/images/branding/googleg/1x/googleg_standard_color_128dp.png\" itemprop=\"image\"><title>Google</title><script nonce=\"ub0BbTHn2PkZMSGThOLrLw\">(function(){window.google={kEI:\\'ynBaZKy-O4T41sQPpPag-Ak\\',kEXPI:\\'0,791104,568305,6059,206,4804,2316,383,246,5,1129120,1197737,664,380089,16115,28684,22430,1362,12320,2814,14765,4998,13228,3847,38444,2872,2891,8349,3405,606,29842,826,30021,15757,3,576,20583,4,1528,2304,29062,13064,13659,4437,16786,5812,2545,4094,7596,1,42154,2,14022,2373,23366,5679,1021,31121,4569,6258,23418,1252,5835,14968,4332,7484,445,2,2,1,26632,8155,7381,15970,872,19633,8,1922,9779,36154,6305,2007,1135,16633,423,5797,11,14329,14,82,13250,6956,3370,5007,18960,281,5122,1506,1524,5628,483,5040,4665,1804,10472,2884,7656,12085,7422,2393,1143,43,2984,476,776,382,2193,3612,2,974,1167,312,174,867,3115,1442,1129,5903,1441,670,521,95,661,4,437,6,8,475,1492,149,3207,278,2773,164,412,1380,598,2825,3,1074,2,295,1646,172,1744,823,168,773,199,512,135,284,622,228,758,334,818,1957,22,15,427,204,436,3,318,45,209,190,198,567,581,561,372,433,776,2,10,535,120,1877,5206641,212,50,135,329,8793789,4590,3311,141,795,19736,1,298,48,5081,12,38,9,480,9,2,7,56,42,126,23944622,398,4041745,1964,16672,2894,6250,14712,1027,1415057,146987,23612289,677,84,227,47,769,57,533,162,2,13,427,87,177,7,851,22,345,179,242,105,358,2,157,653,722,10,31,168,175,188,471,1,472,93,115,5,1245,580,410,488,137,68,593,434,18,239,55,446,84,198,264,447,113,119,602,389,80,715,812,270,242,2710,1496,6\\',kBL:\\'Xw_a\\',kOPI:89978449};google.sn=\\'webhp\\';google.kHL=\\'pt-BR\\';})();(function(){\\nvar h=this||self;function l(){return void 0!==window.google&&void 0!==window.google.kOPI&&0!==window.google.kOPI?window.google.kOPI:null};var m,n=[];function p(a){for(var b;a&&(!a.getAttribute||!(b=a.getAttribute(\"eid\")));)a=a.parentNode;return b||m}function q(a){for(var b=null;a&&(!a.getAttribute||!(b=a.getAttribute(\"leid\")));)a=a.parentNode;return b}function r(a){/^http:/i.test(a)&&\"https:\"===window.location.protocol&&(google.ml&&google.ml(Error(\"a\"),!1,{src:a,glmm:1}),a=\"\");return a}\\nfunction t(a,b,c,d,k){var e=\"\";-1===b.search(\"&ei=\")&&(e=\"&ei=\"+p(d),-1===b.search(\"&lei=\")&&(d=q(d))&&(e+=\"&lei=\"+d));d=\"\";var g=-1===b.search(\"&cshid=\")&&\"slh\"!==a,f=[];f.push([\"zx\",Date.now().toString()]);h._cshid&&g&&f.push([\"cshid\",h._cshid]);c=c();null!=c&&f.push([\"opi\",c.toString()]);for(c=0;c<f.length;c++){if(0===c||0<c)d+=\"&\";d+=f[c][0]+\"=\"+f[c][1]}return\"/\"+(k||\"gen_204\")+\"?atyp=i&ct=\"+String(a)+\"&cad=\"+(b+e+d)};m=google.kEI;google.getEI=p;google.getLEI=q;google.ml=function(){return null};google.log=function(a,b,c,d,k,e){e=void 0===e?l:e;c||(c=t(a,b,e,d,k));if(c=r(c)){a=new Image;var g=n.length;n[g]=a;a.onerror=a.onload=a.onabort=function(){delete n[g]};a.src=c}};google.logUrl=function(a,b){b=void 0===b?l:b;return t(\"\",a,b)};}).call(this);(function(){google.y={};google.sy=[];google.x=function(a,b){if(a)var c=a.id;else{do c=Math.random();while(google.y[c])}google.y[c]=[a,b];return!1};google.sx=function(a){google.sy.push(a)};google.lm=[];google.plm=function(a){google.lm.push.apply(google.lm,a)};google.lq=[];google.load=function(a,b,c){google.lq.push([[a],b,c])};google.loadAll=function(a,b){google.lq.push([a,b])};google.bx=!1;google.lx=function(){};}).call(this);google.f={};(function(){\\ndocument.documentElement.addEventListener(\"submit\",function(b){var a;if(a=b.target){var c=a.getAttribute(\"data-submitfalse\");a=\"1\"===c||\"q\"===c&&!a.elements.q.value?!0:!1}else a=!1;a&&(b.preventDefault(),b.stopPropagation())},!0);document.documentElement.addEventListener(\"click\",function(b){var a;a:{for(a=b.target;a&&a!==document.documentElement;a=a.parentElement)if(\"A\"===a.tagName){a=\"1\"===a.getAttribute(\"data-nohref\");break a}a=!1}a&&b.preventDefault()},!0);}).call(this);</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}\\n</style><style>body,td,a,p,.h{font-family:arial,sans-serif}body{margin:0;overflow-y:scroll}#gog{padding:3px 8px 0}td{line-height:.8em}.gac_m td{line-height:17px}form{margin-bottom:20px}.h{color:#1558d6}em{font-weight:bold;font-style:normal}.lst{height:25px;width:496px}.gsfi,.lst{font:18px arial,sans-serif}.gsfs{font:17px arial,sans-serif}.ds{display:inline-box;display:inline-block;margin:3px 0 4px;margin-left:4px}input{font-family:inherit}body{background:#fff;color:#000}a{color:#4b11a8;text-decoration:none}a:hover,a:active{text-decoration:underline}.fl a{color:#1558d6}a:visited{color:#4b11a8}.sblc{padding-top:5px}.sblc a{display:block;margin:2px 0;margin-left:13px;font-size:11px}.lsbb{background:#f8f9fa;border:solid 1px;border-color:#dadce0 #70757a #70757a #dadce0;height:30px}.lsbb{display:block}#WqQANb a{display:inline-block;margin:0 12px}.lsb{background:url(/images/nav_logo229.png) 0 -261px repeat-x;border:none;color:#000;cursor:pointer;height:30px;margin:0;outline:0;font:15px arial,sans-serif;vertical-align:top}.lsb:active{background:#dadce0}.lst:focus{outline:none}.Ucigb{width:458px}</style><script nonce=\"ub0BbTHn2PkZMSGThOLrLw\">(function(){window.google.erd={jsr:1,bv:1789,de:true};\\nvar h=this||self;var k,l=null!=(k=h.mei)?k:1,n,p=null!=(n=h.sdo)?n:!0,q=0,r,t=google.erd,v=t.jsr;google.ml=function(a,b,d,m,e){e=void 0===e?2:e;b&&(r=a&&a.message);if(google.dl)return google.dl(a,e,d),null;if(0>v){window.console&&console.error(a,d);if(-2===v)throw a;b=!1}else b=!a||!a.message||\"Error loading script\"===a.message||q>=l&&!m?!1:!0;if(!b)return null;q++;d=d||{};b=encodeURIComponent;var c=\"/gen_204?atyp=i&ei=\"+b(google.kEI);google.kEXPI&&(c+=\"&jexpid=\"+b(google.kEXPI));c+=\"&srcpg=\"+b(google.sn)+\"&jsr=\"+b(t.jsr)+\"&bver=\"+b(t.bv);var f=a.lineNumber;void 0!==f&&(c+=\"&line=\"+f);var g=\\na.fileName;g&&(0<g.indexOf(\"-extension:/\")&&(e=3),c+=\"&script=\"+b(g),f&&g===window.location.href&&(f=document.documentElement.outerHTML.split(\"\\\\n\")[f],c+=\"&cad=\"+b(f?f.substring(0,300):\"No script found.\")));c+=\"&jsel=\"+e;for(var u in d)c+=\"&\",c+=b(u),c+=\"=\",c+=b(d[u]);c=c+\"&emsg=\"+b(a.name+\": \"+a.message);c=c+\"&jsst=\"+b(a.stack||\"N/A\");12288<=c.length&&(c=c.substr(0,12288));a=c;m||google.log(0,\"\",a);return a};window.onerror=function(a,b,d,m,e){r!==a&&(a=e instanceof Error?e:Error(a),void 0===d||\"lineNumber\"in a||(a.lineNumber=d),void 0===b||\"fileName\"in a||(a.fileName=b),google.ml(a,!1,void 0,!1,\"SyntaxError\"===a.name||\"SyntaxError\"===a.message.substring(0,11)||-1!==a.message.indexOf(\"Script error\")?3:0));r=null;p&&q>=l&&(window.onerror=null)};})();</script></head><body bgcolor=\"#fff\"><script nonce=\"ub0BbTHn2PkZMSGThOLrLw\">(function(){var src=\\'/images/nav_logo229.png\\';var iesg=false;document.body.onload = function(){window.n && window.n();if (document.images){new Image().src=src;}\\nif (!iesg){document.f&&document.f.q.focus();document.gbqf&&document.gbqf.q.focus();}\\n}\\n})();</script><div id=\"mngb\"><div id=gbar><nobr><b class=gb1>Pesquisa</b> <a class=gb1 href=\"https://www.google.com.br/imghp?hl=pt-BR&tab=wi\">Imagens</a> <a class=gb1 href=\"https://maps.google.com.br/maps?hl=pt-BR&tab=wl\">Maps</a> <a class=gb1 href=\"https://play.google.com/?hl=pt-BR&tab=w8\">Play</a> <a class=gb1 href=\"https://www.youtube.com/?tab=w1\">YouTube</a> <a class=gb1 href=\"https://news.google.com/?tab=wn\">Not\\xedcias</a> <a class=gb1 href=\"https://mail.google.com/mail/?tab=wm\">Gmail</a> <a class=gb1 href=\"https://drive.google.com/?tab=wo\">Drive</a> <a class=gb1 style=\"text-decoration:none\" href=\"https://www.google.com.br/intl/pt-BR/about/products?tab=wh\"><u>Mais</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a href=\"http://www.google.com.br/history/optout?hl=pt-BR\" class=gb4>Hist\\xf3rico da Web</a> | <a  href=\"/preferences?hl=pt-BR\" class=gb4>Configura\\xe7\\xf5es</a> | <a target=_top id=gb_70 href=\"https://accounts.google.com/ServiceLogin?hl=pt-BR&passive=true&continue=https://www.google.com/&ec=GAZAAQ\" class=gb4>Fazer login</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div></div><center><br clear=\"all\" id=\"lgpd\"><div id=\"lga\"><img alt=\"Google\" height=\"92\" src=\"/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png\" style=\"padding:28px 0 14px\" width=\"272\" id=\"hplogo\"><br><br></div><form action=\"/search\" name=\"f\"><table cellpadding=\"0\" cellspacing=\"0\"><tr valign=\"top\"><td width=\"25%\">&nbsp;</td><td align=\"center\" nowrap=\"\"><input name=\"ie\" value=\"ISO-8859-1\" type=\"hidden\"><input value=\"pt-BR\" name=\"hl\" type=\"hidden\"><input name=\"source\" type=\"hidden\" value=\"hp\"><input name=\"biw\" type=\"hidden\"><input name=\"bih\" type=\"hidden\"><div class=\"ds\" style=\"height:32px;margin:4px 0\"><div style=\"position:relative;zoom:1\"><input class=\"lst Ucigb\" style=\"margin:0;padding:5px 8px 0 6px;vertical-align:top;color:#000;padding-right:38px\" autocomplete=\"off\" value=\"\" title=\"Pesquisa Google\" maxlength=\"2048\" name=\"q\" size=\"57\"><img src=\"/textinputassistant/tia.png\" style=\"position:absolute;cursor:pointer;right:5px;top:4px;z-index:300\" data-script-url=\"/textinputassistant/11/pt-BR_tia.js\" id=\"tsuid_1\" alt=\"\" height=\"23\" width=\"27\"><script nonce=\"ub0BbTHn2PkZMSGThOLrLw\">(function(){var id=\\'tsuid_1\\';document.getElementById(id).onclick = function(){var s = document.createElement(\\'script\\');s.src = this.getAttribute(\\'data-script-url\\');(document.getElementById(\\'xjsc\\')||document.body).appendChild(s);};})();</script></div></div><br style=\"line-height:0\"><span class=\"ds\"><span class=\"lsbb\"><input class=\"lsb\" value=\"Pesquisa Google\" name=\"btnG\" type=\"submit\"></span></span><span class=\"ds\"><span class=\"lsbb\"><input class=\"lsb\" id=\"tsuid_2\" value=\"Estou com sorte\" name=\"btnI\" type=\"submit\"><script nonce=\"ub0BbTHn2PkZMSGThOLrLw\">(function(){var id=\\'tsuid_2\\';document.getElementById(id).onclick = function(){if (this.form.q.value){this.checked = 1;if (this.form.iflsig)this.form.iflsig.disabled = false;}\\nelse top.location=\\'/doodles/\\';};})();</script><input value=\"AOEireoAAAAAZFp-2ly9QZ5XfkFMXwCTTUvW9OARMCEE\" name=\"iflsig\" type=\"hidden\"></span></span></td><td class=\"fl sblc\" align=\"left\" nowrap=\"\" width=\"25%\"><a href=\"/advanced_search?hl=pt-BR&amp;authuser=0\">Pesquisa avan\\xe7ada</a></td></tr></table><input id=\"gbv\" name=\"gbv\" type=\"hidden\" value=\"1\"><script nonce=\"ub0BbTHn2PkZMSGThOLrLw\">(function(){var a,b=\"1\";if(document&&document.getElementById)if(\"undefined\"!=typeof XMLHttpRequest)b=\"2\";else if(\"undefined\"!=typeof ActiveXObject){var c,d,e=[\"MSXML2.XMLHTTP.6.0\",\"MSXML2.XMLHTTP.3.0\",\"MSXML2.XMLHTTP\",\"Microsoft.XMLHTTP\"];for(c=0;d=e[c++];)try{new ActiveXObject(d),b=\"2\"}catch(h){}}a=b;if(\"2\"==a&&-1==location.search.indexOf(\"&gbv=2\")){var f=google.gbvu,g=document.getElementById(\"gbv\");g&&(g.value=a);f&&window.setTimeout(function(){location.href=f},0)};}).call(this);</script></form><div id=\"gac_scont\"></div><div style=\"font-size:83%;min-height:3.5em\"><br></div><span id=\"footer\"><div style=\"font-size:10pt\"><div style=\"margin:19px auto;text-align:center\" id=\"WqQANb\"><a href=\"/intl/pt-BR/ads/\">Publicidade</a><a href=\"/services/\">Solu\\xe7\\xf5es empresariais</a><a href=\"/intl/pt-BR/about.html\">Sobre o Google</a><a href=\"https://www.google.com/setprefdomain?prefdom=BR&amp;prev=https://www.google.com.br/&amp;sig=K_D8i31wWDu2Y3HFMMY5dtH9nLgDs%3D\">Google.com.br</a></div></div><p style=\"font-size:8pt;color:#70757a\">&copy; 2023 - <a href=\"/intl/pt-BR/policies/privacy/\">Privacidade</a> - <a href=\"/intl/pt-BR/policies/terms/\">Termos</a></p></span></center><script nonce=\"ub0BbTHn2PkZMSGThOLrLw\">(function(){window.google.cdo={height:757,width:1440};(function(){var a=window.innerWidth,b=window.innerHeight;if(!a||!b){var c=window.document,d=\"CSS1Compat\"==c.compatMode?c.documentElement:c.body;a=d.clientWidth;b=d.clientHeight}\\nif(a&&b&&(a!=google.cdo.width||b!=google.cdo.height)){var e=google,f=e.log,g=\"/client_204?&atyp=i&biw=\"+a+\"&bih=\"+b+\"&ei=\"+google.kEI,h=\"\",k=[],l=void 0!==window.google&&void 0!==window.google.kOPI&&0!==window.google.kOPI?window.google.kOPI:null;null!=l&&k.push([\"opi\",l.toString()]);for(var m=0;m<k.length;m++){if(0===m||0<m)h+=\"&\";h+=k[m][0]+\"=\"+k[m][1]}f.call(e,\"\",\"\",g+h)};}).call(this);})();</script> <script nonce=\"ub0BbTHn2PkZMSGThOLrLw\">(function(){google.xjs={ck:\\'xjs.hp.vUsZk7fd8do.L.X.O\\',cs:\\'ACT90oHvNKf0VndSCQCbyvBm-VFWBpF2tQ\\',excm:[]};})();</script>  <script nonce=\"ub0BbTHn2PkZMSGThOLrLw\">(function(){var u=\\'/xjs/_/js/k\\\\x3dxjs.hp.en.CULboG5sOro.O/am\\\\x3dAAAA6AQAUABgAQ/d\\\\x3d1/ed\\\\x3d1/rs\\\\x3dACT90oFZxVOwITafXDZCzqnHTQ0_Y7i2BA/m\\\\x3dsb_he,d\\';var amd=0;\\nvar e=this||self,f=function(c){return c};var g;var k=function(c){this.g=c};k.prototype.toString=function(){return this.g+\"\"};var m={};\\nfunction q(){var c=u,n=function(){};google.lx=google.stvsc?n:function(){google.timers&&google.timers.load&&google.tick&&google.tick(\"load\",\"xjsls\");var a=document;var b=\"SCRIPT\";\"application/xhtml+xml\"===a.contentType&&(b=b.toLowerCase());b=a.createElement(b);a=null===c?\"null\":void 0===c?\"undefined\":c;if(void 0===g){var d=null;var l=e.trustedTypes;if(l&&l.createPolicy){try{d=l.createPolicy(\"goog#html\",{createHTML:f,createScript:f,createScriptURL:f})}catch(r){e.console&&e.console.error(r.message)}g=\\nd}else g=d}a=(d=g)?d.createScriptURL(a):a;a=new k(a,m);b.src=a instanceof k&&a.constructor===k?a.g:\"type_error:TrustedResourceUrl\";var h,p;(h=(a=null==(p=(h=(b.ownerDocument&&b.ownerDocument.defaultView||window).document).querySelector)?void 0:p.call(h,\"script[nonce]\"))?a.nonce||a.getAttribute(\"nonce\")||\"\":\"\")&&b.setAttribute(\"nonce\",h);document.body.appendChild(b);google.psa=!0;google.lx=n};google.bx||google.lx()};google.xjsu=u;e._F_jsUrl=u;setTimeout(function(){0<amd?google.caft(function(){return q()},amd):q()},0);})();window._ = window._ || {};window._DumpException = _._DumpException = function(e){throw e;};window._s = window._s || {};_s._DumpException = _._DumpException;window._qs = window._qs || {};_qs._DumpException = _._DumpException;function _F_installCss(c){}\\n(function(){google.jl={blt:\\'none\\',chnk:0,dw:false,dwu:true,emtn:0,end:0,ico:false,ikb:0,ine:false,injs:\\'none\\',injt:0,injth:0,injv2:false,lls:\\'default\\',pdt:0,rep:0,snet:true,strt:0,ubm:false,uwp:true};})();(function(){var pmc=\\'{\\\\x22d\\\\x22:{},\\\\x22sb_he\\\\x22:{\\\\x22agen\\\\x22:true,\\\\x22cgen\\\\x22:true,\\\\x22client\\\\x22:\\\\x22heirloom-hp\\\\x22,\\\\x22dh\\\\x22:true,\\\\x22ds\\\\x22:\\\\x22\\\\x22,\\\\x22fl\\\\x22:true,\\\\x22host\\\\x22:\\\\x22google.com\\\\x22,\\\\x22jsonp\\\\x22:true,\\\\x22msgs\\\\x22:{\\\\x22cibl\\\\x22:\\\\x22Limpar pesquisa\\\\x22,\\\\x22dym\\\\x22:\\\\x22Voc\\\\\\\\u00ea quis dizer:\\\\x22,\\\\x22lcky\\\\x22:\\\\x22Estou com sorte\\\\x22,\\\\x22lml\\\\x22:\\\\x22Saiba mais\\\\x22,\\\\x22psrc\\\\x22:\\\\x22Esta pesquisa foi removida do seu\\\\\\\\u003Ca href\\\\x3d\\\\\\\\\\\\x22/history\\\\\\\\\\\\x22\\\\\\\\u003EHist\\\\\\\\u00f3rico da web\\\\\\\\u003C/a\\\\\\\\u003E\\\\x22,\\\\x22psrl\\\\x22:\\\\x22Remover\\\\x22,\\\\x22sbit\\\\x22:\\\\x22Pesquisa por imagem\\\\x22,\\\\x22srch\\\\x22:\\\\x22Pesquisa Google\\\\x22},\\\\x22ovr\\\\x22:{},\\\\x22pq\\\\x22:\\\\x22\\\\x22,\\\\x22rfs\\\\x22:[],\\\\x22sbas\\\\x22:\\\\x220 3px 8px 0 rgba(0,0,0,0.2),0 0 0 1px rgba(0,0,0,0.08)\\\\x22,\\\\x22stok\\\\x22:\\\\x22OrlHPU-Zf45Y4tMwq-07w_ANe2k\\\\x22}}\\';google.pmc=JSON.parse(pmc);})();</script>       </body></html>'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"https://www.google.com\")\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse exemplo, estamos fazendo uma requisição HTTP usando o método GET para acessar a página inicial do Google. O método requests.get() retorna um objeto do tipo Response, que contém várias informações sobre a resposta recebida do servidor, como o código de status, o conteúdo da resposta e os headers.\n",
    "\n",
    "Estamos imprimindo o conteúdo da resposta com o método content, que retorna o conteúdo da resposta em bytes. Podemos manipular esse conteúdo de várias formas, como converter para texto, analisar com a biblioteca BeautifulSoup, ou salvar em um arquivo.\n",
    "\n",
    "A biblioteca Requests também permite enviar dados através de formulários, autenticar em servidores, e outros recursos que nos permitem explorar a web de forma automatizada.\n",
    "\n",
    "Lembrando que ao fazer requisições HTTP, é importante estar ciente das políticas de uso de cada website, pois algumas podem proibir o acesso automatizado ou impor limites de acesso. É importante respeitar essas políticas para evitar problemas legais ou bloqueios de acesso."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 O que é uma requisição HTTP?\n",
    "\n",
    "O protocolo HTTP (Hypertext Transfer Protocol) é um protocolo utilizado na comunicação entre clientes e servidores na web. As requisições HTTP são utilizadas para solicitar dados e informações de um servidor web para serem exibidos em um navegador.\n",
    "\n",
    "Basicamente, uma requisição HTTP consiste em um cabeçalho e um corpo. O cabeçalho contém informações sobre a requisição, como o tipo de método utilizado (GET, POST, PUT, DELETE, etc.), a URL do servidor, e informações sobre o cliente que está fazendo a requisição. O corpo da requisição é opcional e é utilizado para enviar dados do cliente para o servidor, como informações de formulário.\n",
    "\n",
    "Por exemplo, se você digitar uma URL em seu navegador, o navegador envia uma requisição HTTP para o servidor do site correspondente, solicitando a página da web. O servidor responde a essa requisição, enviando o HTML da página de volta para o navegador, que o exibe para você.\n",
    "\n",
    "É importante entender as requisições HTTP para trabalhar com Web Scraping, pois muitas vezes precisamos enviar requisições para obter os dados que queremos raspar de um site. A biblioteca requests em Python é uma ferramenta muito útil para enviar e receber requisições HTTP em projetos de Web Scraping."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Biblioteca requests em Python\n",
    "\n",
    "A biblioteca requests é uma das mais populares para trabalhar com requisições HTTP em Python. Com ela, podemos facilmente fazer requisições GET, POST, PUT, DELETE, entre outras, e manipular os dados que recebemos de volta.\n",
    "\n",
    "Para utilizá-la, primeiro precisamos instalá-la. Podemos fazer isso pelo gerenciador de pacotes pip, digitando no terminal ou prompt de comando: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (2.25.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de instalada, podemos importá-la em nosso código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fazer uma requisição HTTP com o método GET, basta utilizar a função requests.get(url), onde url é a URL do recurso que queremos acessar. Por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://api.github.com/users/octocat\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo, estamos acessando a API do GitHub para obter informações do usuário \"octocat\". O objeto response contém os dados que foram retornados pela requisição.\n",
    "\n",
    "Podemos então manipular os dados recebidos. Por exemplo, para obter o conteúdo da resposta como uma string, podemos utilizar o atributo text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"login\":\"octocat\",\"id\":583231,\"node_id\":\"MDQ6VXNlcjU4MzIzMQ==\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/583231?v=4\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/octocat\",\"html_url\":\"https://github.com/octocat\",\"followers_url\":\"https://api.github.com/users/octocat/followers\",\"following_url\":\"https://api.github.com/users/octocat/following{/other_user}\",\"gists_url\":\"https://api.github.com/users/octocat/gists{/gist_id}\",\"starred_url\":\"https://api.github.com/users/octocat/starred{/owner}{/repo}\",\"subscriptions_url\":\"https://api.github.com/users/octocat/subscriptions\",\"organizations_url\":\"https://api.github.com/users/octocat/orgs\",\"repos_url\":\"https://api.github.com/users/octocat/repos\",\"events_url\":\"https://api.github.com/users/octocat/events{/privacy}\",\"received_events_url\":\"https://api.github.com/users/octocat/received_events\",\"type\":\"User\",\"site_admin\":false,\"name\":\"The Octocat\",\"company\":\"@github\",\"blog\":\"https://github.blog\",\"location\":\"San Francisco\",\"email\":null,\"hireable\":null,\"bio\":null,\"twitter_username\":null,\"public_repos\":8,\"public_gists\":8,\"followers\":9154,\"following\":9,\"created_at\":\"2011-01-25T18:44:36Z\",\"updated_at\":\"2023-04-22T11:18:59Z\"}\n"
     ]
    }
   ],
   "source": [
    "print(response.text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obter o conteúdo como um objeto JSON, podemos utilizar o atributo json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Octocat\n"
     ]
    }
   ],
   "source": [
    "data = response.json()\n",
    "print(data[\"name\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além do método GET, a biblioteca requests também nos permite fazer outras requisições HTTP, como POST, PUT e DELETE. Para isso, basta utilizar as funções requests.post(url), requests.put(url) e requests.delete(url), respectivamente, passando a URL e os dados a serem enviados, se necessário.\n",
    "\n",
    "O uso da biblioteca requests torna a manipulação de requisições HTTP em Python mais fácil e eficiente, e é uma ferramenta importante para o web scraping."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.3 Fazendo uma requisição HTTP em Python\n",
    "\n",
    "Para fazer uma requisição HTTP em Python utilizando a biblioteca requests, é necessário seguir alguns passos simples:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Importar a biblioteca requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Utilizar o método get() para fazer a requisição HTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.example.com')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse exemplo, estamos fazendo uma requisição HTTP para o website https://www.example.com."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Verificar o código de status da resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requisição bem sucedida!\n"
     ]
    }
   ],
   "source": [
    "if response.status_code == 200:\n",
    "    print('Requisição bem sucedida!')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código de status 200 indica que a requisição foi bem sucedida."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Acessar o conteúdo da resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!doctype html>\\n<html>\\n<head>\\n    <title>Example Domain</title>\\n\\n    <meta charset=\"utf-8\" />\\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\\n    <style type=\"text/css\">\\n    body {\\n        background-color: #f0f0f2;\\n        margin: 0;\\n        padding: 0;\\n        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\\n        \\n    }\\n    div {\\n        width: 600px;\\n        margin: 5em auto;\\n        padding: 2em;\\n        background-color: #fdfdff;\\n        border-radius: 0.5em;\\n        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\\n    }\\n    a:link, a:visited {\\n        color: #38488f;\\n        text-decoration: none;\\n    }\\n    @media (max-width: 700px) {\\n        div {\\n            margin: 0 auto;\\n            width: auto;\\n        }\\n    }\\n    </style>    \\n</head>\\n\\n<body>\\n<div>\\n    <h1>Example Domain</h1>\\n    <p>This domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.</p>\\n    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "print(response.content)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conteúdo da resposta pode ser acessado através do atributo content.\n",
    "\n",
    "Além do método get(), a biblioteca requests também possui outros métodos para fazer requisições HTTP, como o post() e o put(), por exemplo. É importante também tratar possíveis erros ao fazer requisições HTTP, utilizando blocos try/except para lidar com exceções que possam ocorrer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Tratando erros com try/except\n",
    "\n",
    "Quando fazemos requisições HTTP em Python, é importante considerar que podem ocorrer erros inesperados durante o processo. Por exemplo, a página que estamos tentando acessar pode estar temporariamente indisponível, ou a conexão com a internet pode estar interrompida.\n",
    "\n",
    "Para lidar com essas situações de erro, podemos utilizar o bloco try/except. O try é o bloco em que colocamos o código que queremos tentar executar, e o except é o bloco em que colocamos o código que queremos executar caso ocorra um erro durante a execução do try.\n",
    "\n",
    "Vejamos um exemplo de como usar o try/except ao fazer uma requisição HTTP com a biblioteca requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requisição bem-sucedida!\n",
      "{\"login\":\"octocat\",\"id\":583231,\"node_id\":\"MDQ6VXNlcjU4MzIzMQ==\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/583231?v=4\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/octocat\",\"html_url\":\"https://github.com/octocat\",\"followers_url\":\"https://api.github.com/users/octocat/followers\",\"following_url\":\"https://api.github.com/users/octocat/following{/other_user}\",\"gists_url\":\"https://api.github.com/users/octocat/gists{/gist_id}\",\"starred_url\":\"https://api.github.com/users/octocat/starred{/owner}{/repo}\",\"subscriptions_url\":\"https://api.github.com/users/octocat/subscriptions\",\"organizations_url\":\"https://api.github.com/users/octocat/orgs\",\"repos_url\":\"https://api.github.com/users/octocat/repos\",\"events_url\":\"https://api.github.com/users/octocat/events{/privacy}\",\"received_events_url\":\"https://api.github.com/users/octocat/received_events\",\"type\":\"User\",\"site_admin\":false,\"name\":\"The Octocat\",\"company\":\"@github\",\"blog\":\"https://github.blog\",\"location\":\"San Francisco\",\"email\":null,\"hireable\":null,\"bio\":null,\"twitter_username\":null,\"public_repos\":8,\"public_gists\":8,\"followers\":9154,\"following\":9,\"created_at\":\"2011-01-25T18:44:36Z\",\"updated_at\":\"2023-04-22T11:18:59Z\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://api.github.com/users/octocat'\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "except requests.exceptions.HTTPError as http_err:\n",
    "    print(f'HTTP error occurred: {http_err}')\n",
    "except Exception as err:\n",
    "    print(f'Other error occurred: {err}')\n",
    "else:\n",
    "    print('Requisição bem-sucedida!')\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP error occurred: 404 Client Error: Not Found for url: https://api.github.com/users/octocat/ide.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://api.github.com/users/octocat/ide.csv'\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "except requests.exceptions.HTTPError as http_err:\n",
    "    print(f'HTTP error occurred: {http_err}')\n",
    "except Exception as err:\n",
    "    print(f'Other error occurred: {err}')\n",
    "else:\n",
    "    print('Requisição bem-sucedida!')\n",
    "    print(response.text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse exemplo, fazemos uma requisição HTTP com a biblioteca requests para o site https://api.github.com/users/octocat. Em seguida, colocamos o código dentro do bloco try. Se a requisição for bem-sucedida, o código dentro do bloco else será executado, e o conteúdo da página será impresso na tela.\n",
    "\n",
    "No entanto, se ocorrer um erro durante a requisição, o bloco except será executado. Se o erro for uma exceção HTTPError, que é uma exceção específica para erros HTTP, o código dentro do primeiro bloco except será executado e uma mensagem de erro HTTP será impressa na tela. Se ocorrer qualquer outro tipo de exceção, o código dentro do segundo bloco except será executado, e uma mensagem genérica de erro será impressa na tela."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Analisando HTML com Beautiful Soup\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 O que é Beautiful Soup?\n",
    "\n",
    "Beautiful Soup é uma biblioteca Python utilizada para extrair informações de arquivos HTML e XML. Ela permite que você analise o conteúdo de uma página da web e encontre elementos específicos com facilidade, tornando mais fácil a criação de um web scraper.\n",
    "\n",
    "A biblioteca recebe uma string contendo o código HTML e a transforma em um objeto que pode ser percorrido e manipulado através de métodos Python. É uma ferramenta bastante flexível, permitindo diferentes estratégias de busca e extração de dados.\n",
    "\n",
    "Além disso, a Beautiful Soup tem uma documentação bem completa e é fácil de aprender, o que a torna uma escolha popular entre desenvolvedores e cientistas de dados que desejam extrair dados de sites da web."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Analisando HTML com Beautiful Soup\n",
    "Beautiful Soup é uma biblioteca Python que nos permite analisar e extrair dados de documentos HTML e XML de maneira fácil e eficiente. Com Beautiful Soup, podemos facilmente buscar e extrair informações específicas de uma página da web.\n",
    "\n",
    "Para analisar um documento HTML com Beautiful Soup, precisamos seguir alguns passos básicos:\n",
    "\n",
    "1 - Fazer uma requisição HTTP para obter o conteúdo HTML da página\n",
    "\n",
    "2 - Criar um objeto BeautifulSoup a partir do conteúdo HTML\n",
    "\n",
    "3 - Utilizar os métodos e atributos do objeto BeautifulSoup para extrair as informações desejadas\n",
    "\n",
    "Vamos ver um exemplo simples de como analisar um documento HTML usando Beautiful Soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Domain\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fazer uma requisição HTTP para obter o conteúdo HTML da página\n",
    "url = 'https://www.example.com'\n",
    "response = requests.get(url)\n",
    "content = response.content\n",
    "\n",
    "# Criar um objeto BeautifulSoup a partir do conteúdo HTML\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Utilizar os métodos e atributos do objeto BeautifulSoup para extrair as informações desejadas\n",
    "title = soup.title.text\n",
    "print(title)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo, estamos fazendo uma requisição HTTP para obter o conteúdo HTML da página https://www.example.com. Em seguida, criamos um objeto BeautifulSoup a partir do conteúdo HTML usando o parser html.parser. Finalmente, utilizamos o atributo title do objeto BeautifulSoup para extrair o título da página e imprimimos na tela."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Buscando elementos HTML específicos\n",
    "\n",
    "Para buscar elementos HTML específicos com o Beautiful Soup, podemos utilizar diferentes métodos e atributos disponíveis na biblioteca.\n",
    "\n",
    "Um dos métodos mais comuns é o find, que busca a primeira ocorrência do elemento especificado. Por exemplo, se quisermos buscar a primeira ocorrência de uma tag h1 em uma página, podemos utilizar o seguinte código:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"ytd-searchbox-spt\" id=\"search-container\" slot=\"search-container\"></div>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://youtube.com.br'\n",
    "resposta = requests.get(url)\n",
    "soup = BeautifulSoup(resposta.content, 'html.parser')\n",
    "\n",
    "titulo = soup.find('div')\n",
    "print(titulo)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código acima faz uma requisição HTTP para a URL especificada e utiliza o Beautiful Soup para analisar o conteúdo HTML retornado. Em seguida, utilizamos o método find para buscar a primeira ocorrência da tag h1. O resultado é armazenado na variável titulo, que pode ser impressa na tela ou utilizada de outras maneiras.\n",
    "\n",
    "Além do método find, existem outros métodos e atributos disponíveis no Beautiful Soup para buscar elementos HTML específicos, como o find_all (para buscar todas as ocorrências de um elemento), o select (para buscar elementos com base em um seletor CSS) e diversos outros."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Tratando erros com try/except\n",
    "\n",
    "Tratar erros é uma parte importante de qualquer programa, e isso inclui o código de web scraping. Às vezes, quando você tenta buscar um elemento específico em uma página, pode ocorrer algum erro, como a página não estar carregando corretamente ou o elemento não existir.\n",
    "\n",
    "Para lidar com esses erros, podemos usar o bloco try/except do Python. Esse bloco permite que o programa tente executar uma determinada ação e, caso ocorra um erro, o programa não pare completamente. Em vez disso, podemos lidar com o erro de maneira apropriada e continuar a execução do programa.\n",
    "\n",
    "Por exemplo, suponha que você esteja buscando um elemento específico em uma página da web usando Beautiful Soup, mas esse elemento pode ou não estar presente na página. Podemos usar o bloco try/except para lidar com essa situação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O elemento não foi encontrado\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://youtube.com.br'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "try:\n",
    "    element = soup.find('div', {'class': 'classe_do_elemento'})\n",
    "    print(element.text)\n",
    "except AttributeError:\n",
    "    print('O elemento não foi encontrado')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse exemplo, estamos tentando buscar um elemento div com a classe classe_do_elemento na página https://www.exemplo.com. Se o elemento for encontrado, o texto dentro dele será impresso. Caso contrário, será impressa a mensagem 'O elemento não foi encontrado'.\n",
    "\n",
    "Ao usar o bloco try/except, estamos tornando nosso código mais robusto e resistente a falhas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Selenium para Web Scraping Dinâmico\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 O que é Web Scraping dinâmico?\n",
    "\n",
    "Web scraping dinâmico é uma técnica usada para coletar dados de páginas da web que possuem conteúdo que é gerado dinamicamente pelo navegador. Diferentemente do web scraping estático, que coleta dados de páginas estáticas em HTML, o web scraping dinâmico coleta dados de páginas que utilizam tecnologias como JavaScript, Ajax e outras que requerem interações com o usuário para carregar o conteúdo.\n",
    "\n",
    "Em outras palavras, o web scraping dinâmico é necessário quando o conteúdo da página que se deseja coletar não é carregado no HTML da página quando a requisição é feita, mas sim depois de a página ter sido carregada, geralmente por meio de solicitações adicionais do servidor ou do uso de JavaScript.\n",
    "\n",
    "Para realizar o web scraping dinâmico, é comum utilizar ferramentas como o Selenium, que permite a automação de interações com o navegador, como clicar em botões, preencher formulários e navegar por páginas. Com isso, é possível emular as ações de um usuário e coletar os dados que são gerados dinamicamente.\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Como o Selenium pode ajudar?\n",
    "\n",
    "\n",
    "O Selenium é uma ferramenta que permite a automação de interações com um navegador web, o que a torna uma ótima opção para o web scraping dinâmico. Com o Selenium, é possível controlar um navegador para interagir com elementos que não podem ser acessados diretamente pelo Beautiful Soup ou pela biblioteca requests, como botões que carregam dados dinamicamente ou formulários que precisam ser preenchidos para acessar informações.\n",
    "\n",
    "O Selenium pode simular interações de um usuário real, como clicar em um botão, preencher um formulário ou rolar a página, para extrair os dados necessários. Além disso, o Selenium também é capaz de lidar com diferentes tipos de autenticação, cookies e sessões.\n",
    "\n",
    "Segue um exemplo de código em Python usando o Selenium para interagir com um site dinâmico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m driver\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mhttps://www.google.com\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Encontra o campo de pesquisa e insere o termo \"Web Scraping\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m search_box \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39;49mfind_element_by_name(\u001b[39m\"\u001b[39m\u001b[39mq\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m search_box\u001b[39m.\u001b[39msend_keys(\u001b[39m\"\u001b[39m\u001b[39mWeb Scraping\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Clica no botão de pesquisa\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_name'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# Define a instância do driver do Selenium\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navega para a página de pesquisa do Google\n",
    "driver.get(\"https://www.google.com\")\n",
    "\n",
    "# Encontra o campo de pesquisa e insere o termo \"Web Scraping\"\n",
    "search_box = driver.find_element_by_name(\"q\")\n",
    "search_box.send_keys(\"Web Scraping\")\n",
    "\n",
    "# Clica no botão de pesquisa\n",
    "search_box.submit()\n",
    "\n",
    "# Espera a página carregar e exibe o título\n",
    "driver.implicitly_wait(10)\n",
    "print(driver.title)\n",
    "\n",
    "# Fecha a instância do driver\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse exemplo abre o navegador Google Chrome e navega para a página de pesquisa do Google. Em seguida, encontra o campo de pesquisa, insere o termo \"Web Scraping\" e clica no botão de pesquisa. Por fim, espera a página carregar e exibe o título da página. É importante notar que esse exemplo requer que o Selenium esteja instalado e que o driver do navegador esteja configurado corretamente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Automatizando tarefas com o Selenium\n",
    "\n",
    "O Selenium é uma ferramenta muito útil para automatizar tarefas de navegação na web. Ele permite que você interaja com páginas web como se estivesse usando um navegador de verdade, clicando em botões, preenchendo formulários, navegando entre páginas e realizando outras ações.\n",
    "\n",
    "Para automatizar uma tarefa com o Selenium, você precisa primeiro criar uma instância do driver do Selenium para o navegador que você deseja usar. Em seguida, você pode usar os métodos fornecidos pelo Selenium para encontrar elementos na página e realizar ações com eles.\n",
    "\n",
    "Por exemplo, se você deseja preencher um formulário e clicar no botão de envio, pode fazer o seguinte usando o Selenium:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProtocolError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:699\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 699\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    700\u001b[0m     conn,\n\u001b[1;32m    701\u001b[0m     method,\n\u001b[1;32m    702\u001b[0m     url,\n\u001b[1;32m    703\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    704\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    705\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    706\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    707\u001b[0m )\n\u001b[1;32m    709\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:445\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    442\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    443\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    444\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    446\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:440\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    444\u001b[0m     \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1375\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line:\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# sending a valid response.\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mraise\u001b[39;00m RemoteDisconnected(\u001b[39m\"\u001b[39m\u001b[39mRemote end closed connection without\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m                              \u001b[39m\"\u001b[39m\u001b[39m response\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m driver \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39mChrome()\n\u001b[1;32m      6\u001b[0m \u001b[39m# Navega até a página do formulário\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m driver\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mhttps://www.exemplo.com/formulario\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Encontra o campo de nome e preenche com \"João\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m nome_input \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39mfind_element_by_name(\u001b[39m\"\u001b[39m\u001b[39mnome\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:449\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET, {\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m: url})\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:438\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m    436\u001b[0m         params[\u001b[39m\"\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_id\n\u001b[0;32m--> 438\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommand_executor\u001b[39m.\u001b[39;49mexecute(driver_command, params)\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_handler\u001b[39m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/selenium/webdriver/remote/remote_connection.py:290\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    288\u001b[0m data \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mdump_json(params)\n\u001b[1;32m    289\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(command_info[\u001b[39m0\u001b[39;49m], url, body\u001b[39m=\u001b[39;49mdata)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/selenium/webdriver/remote/remote_connection.py:311\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    308\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 311\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    312\u001b[0m     statuscode \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus\n\u001b[1;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_url(\n\u001b[1;32m     75\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_encode_body(\n\u001b[1;32m     79\u001b[0m         method, url, fields\u001b[39m=\u001b[39;49mfields, headers\u001b[39m=\u001b[39;49mheaders, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49murlopen_kw\n\u001b[1;32m     80\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m extra_kw[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mupdate(headers)\n\u001b[1;32m    168\u001b[0m extra_kw\u001b[39m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 170\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/poolmanager.py:375\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    373\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m    374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(method, u\u001b[39m.\u001b[39;49mrequest_uri, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    377\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n\u001b[1;32m    378\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:755\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, (SocketError, HTTPException)):\n\u001b[1;32m    753\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 755\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    756\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    757\u001b[0m )\n\u001b[1;32m    758\u001b[0m retries\u001b[39m.\u001b[39msleep()\n\u001b[1;32m    760\u001b[0m \u001b[39m# Keep track of the error for the retry warning.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/util/retry.py:532\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[39melif\u001b[39;00m error \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_read_error(error):\n\u001b[1;32m    530\u001b[0m     \u001b[39m# Read retry?\u001b[39;00m\n\u001b[1;32m    531\u001b[0m     \u001b[39mif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 532\u001b[0m         \u001b[39mraise\u001b[39;00m six\u001b[39m.\u001b[39;49mreraise(\u001b[39mtype\u001b[39;49m(error), error, _stacktrace)\n\u001b[1;32m    533\u001b[0m     \u001b[39melif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    534\u001b[0m         read \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/six.py:718\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    716\u001b[0m         value \u001b[39m=\u001b[39m tp()\n\u001b[1;32m    717\u001b[0m     \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[0;32m--> 718\u001b[0m         \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    719\u001b[0m     \u001b[39mraise\u001b[39;00m value\n\u001b[1;32m    720\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:699\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    698\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 699\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    700\u001b[0m     conn,\n\u001b[1;32m    701\u001b[0m     method,\n\u001b[1;32m    702\u001b[0m     url,\n\u001b[1;32m    703\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    704\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    705\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    706\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    707\u001b[0m )\n\u001b[1;32m    709\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:445\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    440\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    441\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    442\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    443\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    444\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    446\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    447\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:440\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    441\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    442\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    443\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    444\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1375\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mreply:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mrepr\u001b[39m(line))\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line:\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# sending a valid response.\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mraise\u001b[39;00m RemoteDisconnected(\u001b[39m\"\u001b[39m\u001b[39mRemote end closed connection without\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m                              \u001b[39m\"\u001b[39m\u001b[39m response\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     version, status, reason \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(\u001b[39mNone\u001b[39;00m, \u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# Cria uma instância do driver do Chrome\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navega até a página do formulário\n",
    "driver.get(\"https://www.exemplo.com/formulario\")\n",
    "\n",
    "# Encontra o campo de nome e preenche com \"João\"\n",
    "nome_input = driver.find_element_by_name(\"nome\")\n",
    "nome_input.send_keys(\"João\")\n",
    "\n",
    "# Encontra o campo de e-mail e preenche com \"joao@email.com\"\n",
    "email_input = driver.find_element_by_name(\"email\")\n",
    "email_input.send_keys(\"joao@email.com\")\n",
    "\n",
    "# Encontra o botão de envio e clica nele\n",
    "submit_button = driver.find_element_by_xpath(\"//input[@type='submit']\")\n",
    "submit_button.click()\n",
    "\n",
    "# Fecha o navegador\n",
    "driver.quit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este é apenas um exemplo simples, mas você pode usar o Selenium para automatizar muitas outras tarefas, como baixar arquivos, interagir com APIs, extrair informações de páginas web, entre outras. É importante lembrar, no entanto, que o uso do Selenium para scraping pode violar os termos de uso de alguns sites e pode não ser permitido legalmente em alguns casos. Certifique-se de estar familiarizado com as leis e políticas aplicáveis antes de usar o Selenium para automatizar tarefas na web."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Tratando erros com try/except\n",
    "\n",
    "Assim como em outras partes do projeto, é importante tratar possíveis erros que possam ocorrer durante a execução do código com o Selenium. Algumas das situações em que pode ocorrer erros são:\n",
    "\n",
    "O site estar indisponível;\n",
    "O elemento HTML que estamos tentando acessar não existir;\n",
    "O navegador não conseguir carregar completamente a página.\n",
    "Para tratar esses erros, podemos utilizar a estrutura de try/except em conjunto com exceções específicas do Selenium, como a NoSuchElementException (quando o elemento HTML não existe) e a TimeoutException (quando ocorre um timeout no carregamento da página).\n",
    "\n",
    "Um exemplo de código utilizando try/except com Selenium seria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Inicializa o driver do Selenium\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Navega até a página do Google\n",
    "    driver.get(\"https://www.google.com\")\n",
    "\n",
    "    # Encontra o campo de pesquisa e insere o termo \"Web Scraping\"\n",
    "    search_box = driver.find_element(By.NAME, \"q\")\n",
    "    search_box.send_keys(\"Web Scraping\")\n",
    "\n",
    "    # Clica no botão de pesquisa\n",
    "    search_box.submit()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Erro ao executar o script:\", e)\n",
    "\n",
    "finally:\n",
    "    # Encerra o driver do Selenium\n",
    "    driver.quit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse exemplo, estamos utilizando a estrutura de try/except para tratar possíveis exceções do Selenium, como a NoSuchElementException e a TimeoutException. Caso ocorra algum erro, o código exibe uma mensagem de erro com a exceção capturada. Por fim, o driver do Selenium é encerrado na cláusula finally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
